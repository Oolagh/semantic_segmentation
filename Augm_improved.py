# -*- coding: utf-8 -*-
"""Arash_Augm_improved.ipynb

Automatically generated by Colaboratory.

# Semantic Segmentation of the satellite images (including Focal loss method)

*   There are 30 images, 24 masked images (ground truth) and 1 outlier where the masked and image do not match.
*   I used the Convolutional Neural Network U-Net (previous reports suggest that it performs well for semantic segmentation cases, but I am not sure if this is the best choice) 
*   First I tried to train the U-Net using the available images; the images are organized in a main folder, i.e. Pics, and 3 subfolders; train, labels, and test. In each subfolder there are 24, 24, and 5 images respectively. 
*   In the first network, I used the following filters: encoder=[16,32,64,128], 256 , decoder=[128,64,32,16], and [1] for the output layer, but I was not happy with the outcome so I have increased the filters to 
encoder=[16,32,64,128,256,512], 1024 , decoder=[512,256,128,64,32,16], and [1] for the output layer. 
*   Still the result was not very promising, I think mainly because of the low number of training inputs/images (24). I have decided to use some of the augmentation techniques and increase the number of training images and their ground truth files. I am not sure if I was allowed to do that, at anytime we can just change the loading folders/files to feed the model with the original images.
*   The augmentation techniques that I used are small angle rotation, flip left to right and top to bottom, and zoom in. I think the zoom case was not a good choice as it affects the resolution of the images. Each training image and its mask went through the same augmentation process. As I was not sure if I can augment the images on the fly properly, inside the code, I did it separately (using https://github.com/mdbloice/Augmentor) then upload the images to "Google Drive". Total number of images/masks, including the original ones, are 324.  
*   The predicted segmentations are satisfactory in some cases, but in overall I do not get any sharp segmentation with clear boundaries and correct labeling compared to the validation/ground truth images (I have not used any threshold to manipulate the grayscale output). 
*   Another option which I have not tried was the manipulation or training based on the colors, the similarity between the color of the streets and the roofs is challenging and there must be some solutions for that.
*   I could not understand the reason behind covering some part of the images with white rectangles, just to remove partial roofs from images? or to add more complexity?
*   I have used two metrics for comparison; "Accuracy" and "Mean Intersection-Over-Union" and played with different parameters to tune the training. Parameters like the optimizer, the learning rate, the metrics, but still I am learning, so I highly appreciate your tips and tricks.   
*   Something which is still puzzling me is the performance of the Mean Intersection-Over-Union evaluation metric in Keras package; as its name suggests, it should present the best performance among other metrics for semantic segmentation specially in case of binary segmentation, but in my case it does not perform well, probably I missed something, a parameter or additional analysis or option.

**- Loading the required libraries** 

(not all of them are necessary)
"""

pip install tensorflow-addons

# Commented out IPython magic to ensure Python compatibility.
from keras.models import Model, load_model
from keras.layers import Input
from keras.layers.core import Dropout, Lambda
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.layers.pooling import MaxPooling2D
from keras.layers.merge import concatenate
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras import backend as K
from tensorflow.keras.metrics import MeanIoU

import tensorflow as tf
from tensorflow.keras.layers.experimental import preprocessing 
import tensorflow_addons as tfa


import pandas as pd
import numpy as np
import random
import os
import glob
import sys
import cv2

from IPython.display import Image, display
from tensorflow.keras.preprocessing.image import load_img
import PIL
from PIL import ImageOps
from PIL import Image

import matplotlib.pyplot as plt
# %matplotlib inline

from skimage.io import imread, imshow
from skimage.transform import resize

# No Warning Messages!
import warnings
warnings.filterwarnings('ignore')

"""**- Mounting the Google Drive where the data are located.**"""

from google.colab import drive
drive.mount('/content/drive')

"""**- Defining the width, height, and the number of channels of the input images.**"""

IMG_HEIGHT = 256
IMG_WIDTH = 256
IMG_CHANNELS = 3

"""**- Loading input images and the corresponding masks (ground  truth)**"""

#load training and mask images as a list
train_images = []

#for directory_path in glob.glob("/content/drive/MyDrive/Colab Notebooks/Pics/Aug/train"):
#    for img_path in glob.glob(os.path.join(directory_path, "*.png")):
fnames = [img_path for img_path in glob.glob("/content/drive/MyDrive/Colab Notebooks/Pics/Aug/train/*.png")]
fnames.sort()
for img_path in fnames:
        #reading colored images 
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)   
        #option for resize    
        img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))
        #change colors channel order
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        train_images.append(img)
#Convert list to array  
train_images = np.array(train_images)

#mask/label 
train_masks = [] 
fnames = [mask_path for mask_path in glob.glob("/content/drive/MyDrive/Colab Notebooks/Pics/Aug/labels/*.png")]
fnames.sort()
for mask_path in fnames:
#for directory_path in glob.glob("/content/drive/My Drive/Colab Notebooks/Pics/Aug/labels"):
#    for mask_path in glob.glob(os.path.join(directory_path, "*.png")):
        #loading grayscale image     
        mask = cv2.imread(mask_path, 0)       
        mask = cv2.resize(mask, (IMG_HEIGHT, IMG_WIDTH))
        train_masks.append(mask)
train_masks = np.array(train_masks)

train_masks.shape

train_images.shape

"""**- Normalizing the images to [0, 1]**"""

x = np.asarray(train_images, dtype=np.float32)/255
y = np.asarray(train_masks, dtype=np.float32)/255

"""**- Checking the formats**


Make sure that the input, x, and validation, y, have correct dimention; 3 channel colored and 1 channel binary respectively

"""

x = x.reshape(x.shape[0], x.shape[1], x.shape[2], 3)
y = y.reshape(y.shape[0], y.shape[1], y.shape[2], 1)

"""**- Check some of the images and their masks randomly to make sure they match and there is no outliers in the input data.**"""

ids1 = random.sample(range(len(x)-1), 5)

f, axarr = plt.subplots(2,5,figsize=(20, 20))
plt.subplots_adjust(wspace=0.4, hspace=-0.78)

for j in range(5):
   axarr[0,j].imshow(x[ids1[j]])
   axarr[1,j].imshow(y[ids1[j]].squeeze(axis=2), cmap='gray')

"""**- Set aside the validation images**

"""

from sklearn.model_selection import train_test_split

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=0)

x_val.shape

"""**- Building the UNet model;** 


*   Filters are: encoder=[16,32,64,128,256,512], 1024 , decoder=[512,256,128,64,32,16], and [1] for the output layer.
*   activation= Rectified Linear Unit, accompanied with he_normal as kernel initializer.
*   Padding the same as input image (to get the same size output).
*   Using dropout to prevent overfitting and improving the training.
*   Using MaxPooling to reduce the computational cost (I assume MaxPooling works better for images with darker background(?))
*   The convolution kernel sizes are (3, 3), except for the upsampling (transpose) (2, 2) and the final layer (1, 1), strides=1 in all cases.
*   The concatenation is used as defined in the UNet model. 
*   The output layer samples pixel by pixel, filter = 1 and kernel =(1, 1), besides using the "sigmoid" as activation to have the output in the range of [0, 1].   
*   Two metrics have been used for comparison, metrics=['accuracy'] and metrics=[tf.keras.metrics.MeanIoU(num_classes=2)] besides the "adam" optimizer with smaller than default value of learning rate for the optimizer.

"""

#Build the UNet model.
inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))
#if the inputs are not normalized
#s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)

dout=0.3 #Dropout value

#Contraction path, encoder
c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)
c1 = tf.keras.layers.Dropout(dout)(c1)
c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)
p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)

c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)
c2 = tf.keras.layers.Dropout(dout)(c2)
c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)
p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)
 
c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)
c3 = tf.keras.layers.Dropout(dout)(c3)
c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)
p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)
 
c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)
c4 = tf.keras.layers.Dropout(dout)(c4)
c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)
p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)

c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)
c5 = tf.keras.layers.Dropout(dout)(c5)
c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)
p5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c5)

c6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p5)
c6 = tf.keras.layers.Dropout(dout)(c6)
c6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)
p6 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c6)

#Middle

cm = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p6)
cm = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(cm)

#Expansive path, decoder 

u6 = tf.keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(cm)
u6 = tf.keras.layers.concatenate([u6, c6])
cu6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)
cu6 = tf.keras.layers.Dropout(dout)(cu6)
cu6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(cu6)

u5 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(cu6)
u5 = tf.keras.layers.concatenate([u5, c5])
cu5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u5)
cu5 = tf.keras.layers.Dropout(dout)(cu5)
cu5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(cu5)

u4 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(cu5)
u4 = tf.keras.layers.concatenate([u4, c4])
cu4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u4)
cu4 = tf.keras.layers.Dropout(dout)(cu4)
cu4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(cu4)
 
u3 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(cu4)
u3 = tf.keras.layers.concatenate([u3, c3])
cu3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u3)
cu3 = tf.keras.layers.Dropout(dout)(cu3)
cu3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(cu3)
 
u2 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(cu3)
u2 = tf.keras.layers.concatenate([u2, c2])
cu2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u2)
cu2 = tf.keras.layers.Dropout(dout)(cu2)
cu2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(cu2)
 
u1 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(cu2)
u1 = tf.keras.layers.concatenate([u1, c1], axis=3)
cu1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u1)
cu1 = tf.keras.layers.Dropout(dout)(cu1)
cu1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(cu1)
 
outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(cu1)
 
model_Acc = tf.keras.Model(inputs=[inputs], outputs=[outputs])
model_IoU = tf.keras.Model(inputs=[inputs], outputs=[outputs])
model_FL = tf.keras.Model(inputs=[inputs], outputs=[outputs])


model_Acc.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 4e-4), loss='binary_crossentropy', metrics=['accuracy'])

model_IoU.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 6e-4), loss='binary_crossentropy', metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])

#### Using focal loss function
model_FL.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 5e-4), loss=tfa.losses.SigmoidFocalCrossEntropy(alpha=0.2, gamma=2), metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])


#model.summary()

#Focal Loss
#Modelcheckpoint
callbacks_FL = [
        tf.keras.callbacks.EarlyStopping(patience=6, monitor='val_loss'),
        tf.keras.callbacks.TensorBoard(log_dir='drive/MyDrive/Colab Notebooks/Pics/Aug/FocalLosslogs2'),
        tf.keras.callbacks.ModelCheckpoint("drive/MyDrive/Colab Notebooks/Pics/Aug/FocalLoss2.h5", save_best_only=True)]

results_FL = model_Acc.fit(x_train, y_train, validation_split=0.2, batch_size=8, epochs=70, shuffle=True, callbacks=callbacks_FL)

"""**- Metrics=Mean_IoU**"""

#IoU
#Modelcheckpoint
callbacks_IoU = [
        tf.keras.callbacks.EarlyStopping(patience=6, monitor='val_loss'),
        tf.keras.callbacks.TensorBoard(log_dir='drive/MyDrive/Colab Notebooks/Pics/Aug/IoUlogs'),
        tf.keras.callbacks.ModelCheckpoint("drive/MyDrive/Colab Notebooks/Pics/Aug/IoU.h5", save_best_only=True)]
        

results_IoU = model_IoU.fit(x_train, y_train, validation_split=0.2, batch_size=8, epochs=70, shuffle=True, callbacks=callbacks_IoU)

"""**- Metrics=Accuracy**"""

#Accuracy
#Modelcheckpoint
callbacks_Acc = [
        tf.keras.callbacks.EarlyStopping(patience=6, monitor='val_loss'),
        tf.keras.callbacks.TensorBoard(log_dir='drive/MyDrive/Colab Notebooks/Pics/Aug/Acclogs'),
        tf.keras.callbacks.ModelCheckpoint("drive/MyDrive/Colab Notebooks/Pics/Aug/Acc.h5", save_best_only=True)]

results_Acc = model_Acc.fit(x_train, y_train, validation_split=0.2, batch_size=8, epochs=70, shuffle=True, callbacks=callbacks_Acc)

print(results_IoU.history.keys())

"""# Results

**- Loading the trained models**
"""

# IoU
model_IoU.load_weights('drive/MyDrive/Colab Notebooks/Pics/Aug/IoU.h5')
y_pred_IoU = model_IoU.predict(x_val)
# Acc
model_Acc.load_weights('drive/MyDrive/Colab Notebooks/Pics/Aug/Acc.h5')
y_pred_Acc = model_Acc.predict(x_val)
#FL
model_FL.load_weights('drive/MyDrive/Colab Notebooks/Pics/Aug/FocalLoss2.h5')
y_pred_FL = model_FL.predict(x_val)

#Defining a threshold for better visualization of the predicted outputs. 
#pred_fI = (y_pred_IoU > 0.2).astype(np.uint8)
#pred_fA = (y_pred_Acc > 0.3).astype(np.uint8)

"""**- Comparing the results of the two models,metrics= Mean_IoU and Accuracy, with the ground truth**"""

ids = random.sample(range(len(x_val)-1), 5)

f, axarr = plt.subplots(5,5,figsize=(25, 25))
plt.subplots_adjust(wspace=-0.6, hspace=0.3)

for j in range(5):
   axarr[j,0].imshow(x_val[ids[j]])
   axarr[j,1].imshow(y_val[ids[j]].squeeze(axis=2), cmap='gray')
   axarr[j,2].imshow(y_pred_IoU[ids[j]].squeeze(axis=2), cmap='gray')
   axarr[j,3].imshow(y_pred_Acc[ids[j]].squeeze(axis=2), cmap='gray')
   axarr[j,4].imshow(y_pred_FL[ids[j]].squeeze(axis=2), cmap='gray')
   if j==0:
     axarr[0,0].set_title('Image', fontsize=20)
     axarr[0,1].set_title('Ground truth', fontsize=20)
     axarr[0,2].set_title('MeanIoU', fontsize=20)
     axarr[0,3].set_title('Accuracy', fontsize=20)
     axarr[0,4].set_title('Focal Loss', fontsize=20)

"""**Plots of the loss and accuracy of the trained networks**


"""

#IoU
loss = results_IoU.history['loss']
val_loss = results_IoU.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#Accuracy
loss = results_Acc.history['loss']
val_loss = results_Acc.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc = results_Acc.history['accuracy']
val_acc = results_Acc.history['val_accuracy']
plt.plot(epochs, acc, 'y', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#Focal Loss
loss = results_FL.history['loss']
val_loss = results_FL.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc = results_FL.history['accuracy']
val_acc = results_FL.history['val_accuracy']
plt.plot(epochs, acc, 'y', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""**- Trying the trained networks on the test images, without ground truth (5 images)**"""

#Check with test images, without having the ground truth
test_images = []

for directory_path in glob.glob("/content/drive/MyDrive/Colab Notebooks/Pics/Aug/test"):
    for img_path in glob.glob(os.path.join(directory_path, "*.png")):
        #print(img_path)
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       
        img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        test_images.append(img)
        #train_labels.append(label)
#Convert list to array for machine learning processing        
test_images = np.array(test_images)

xt = np.asarray(test_images, dtype=np.float32)/255

xt = xt.reshape(xt.shape[0], xt.shape[1], xt.shape[2], 3)

model_IoU.load_weights('drive/MyDrive/Colab Notebooks/Pics/Aug/IoU.h5')
pred_IoUt = model_IoU.predict(xt)
model_Acc.load_weights('drive/MyDrive/Colab Notebooks/Pics/Aug/Acc.h5')
pred_Acct = model_Acc.predict(xt)
model_FL.load_weights('drive/MyDrive/Colab Notebooks/Pics/Aug/FocalLoss2.h5')
pred_FLt = model_FL.predict(xt)

f, axarr = plt.subplots(5,4,figsize=(25, 25))
plt.subplots_adjust(wspace=-0.6, hspace=0.3)

for j in range(5):
   axarr[j,0].imshow(xt[j])
   axarr[j,1].imshow(pred_IoUt[j].squeeze(axis=2), cmap='gray')
   axarr[j,2].imshow(pred_Acct[j].squeeze(axis=2), cmap='gray')
   axarr[j,3].imshow(pred_FLt[j].squeeze(axis=2), cmap='gray')
   if j==0:
     axarr[0,0].set_title('Image', fontsize=20)
     axarr[0,1].set_title('MeanIoU', fontsize=20)
     axarr[0,2].set_title('Accuracy', fontsize=20)
     axarr[0,3].set_title('Focal Loss', fontsize=20)
